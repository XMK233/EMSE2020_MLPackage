{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script, some raw classification for families will be done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how many models have multiple versions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 383 modules in TFHub\n",
      "\n",
      "\n",
      "\n",
      "Wiki-words-250-with-normalization@@google [1, 2]\n",
      "Wiki-words-250@@google [1, 2]\n",
      "Wiki-words-500-with-normalization@@google [1, 2]\n",
      "Wiki-words-500@@google [1, 2]\n",
      "albert_base@@google [1, 2, 3]\n",
      "albert_en_base@@tensorflow [1]\n",
      "albert_en_large@@tensorflow [1]\n",
      "albert_en_xlarge@@tensorflow [1]\n",
      "albert_en_xxlarge@@tensorflow [1]\n",
      "albert_large@@google [1, 2, 3]\n",
      "albert_lite_base@@tensorflow [1]\n",
      "albert_xlarge@@google [2, 3, 1]\n",
      "albert_xxlarge@@google [1, 2, 3]\n",
      "bert-uncased-tf2-qa@@see-- [1]\n",
      "bert_cased_L-12_H-768_A-12@@google [1]\n",
      "bert_cased_L-24_H-1024_A-16@@google [1]\n",
      "bert_chinese_L-12_H-768_A-12@@google [1]\n",
      "bert_en_cased_L-12_H-768_A-12@@tensorflow [1]\n",
      "bert_en_cased_L-24_H-1024_A-16@@tensorflow [1]\n",
      "bert_en_uncased_L-12_H-768_A-12@@tensorflow [1]\n",
      "bert_en_uncased_L-24_H-1024_A-16@@tensorflow [1]\n",
      "bert_en_wwm_cased_L-24_H-1024_A-16@@tensorflow [1]\n",
      "bert_en_wwm_uncased_L-24_H-1024_A-16@@tensorflow [1]\n",
      "bert_multi_cased_L-12_H-768_A-12@@google [1]\n",
      "bert_multi_cased_L-12_H-768_A-12@@tensorflow [1]\n",
      "bert_uncased_L-12_H-768_A-12@@google [1]\n",
      "bert_uncased_L-24_H-1024_A-16@@google [1]\n",
      "bert_zh_L-12_H-768_A-12@@tensorflow [1]\n",
      "bigbigan-resnet50@@deepmind [1]\n",
      "bigbigan-revnet50x4@@deepmind [1]\n",
      "biggan-128@@deepmind [1, 2]\n",
      "biggan-256@@deepmind [1, 2]\n",
      "biggan-512@@deepmind [1, 2]\n",
      "biggan-deep-128@@deepmind [1]\n",
      "biggan-deep-256@@deepmind [1]\n",
      "biggan-deep-512@@deepmind [1]\n",
      "compare_gan-model_10_lsun_bedroom_resnet19@@google [1]\n",
      "compare_gan-model_11_cifar10_resnet_cifar@@google [1]\n",
      "compare_gan-model_12_cifar10_resnet_cifar@@google [1]\n",
      "compare_gan-model_13_cifar10_resnet_cifar@@google [1]\n",
      "compare_gan-model_14_cifar10_resnet_cifar@@google [1]\n",
      "compare_gan-model_15_cifar10_resnet_cifar@@google [1]\n",
      "compare_gan-model_1_celebahq128_resnet19@@google [1]\n",
      "compare_gan-model_2_celebahq128_resnet19@@google [1]\n",
      "compare_gan-model_3_lsun_bedroom_resnet19@@google [1]\n",
      "compare_gan-model_4_lsun_bedroom_resnet19@@google [1]\n",
      "compare_gan-model_5_celebahq128_resnet19@@google [1]\n",
      "compare_gan-model_6_celebahq128_resnet19@@google [1]\n",
      "compare_gan-model_7_lsun_bedroom_resnet19@@google [1]\n",
      "compare_gan-model_8_lsun_bedroom_resnet19@@google [1]\n",
      "compare_gan-model_9_celebahq128_resnet19@@google [1]\n",
      "compare_gan-s3gan_10_128x128@@google [1]\n",
      "compare_gan-s3gan_10_256x256@@google [1]\n",
      "compare_gan-s3gan_20_128x128@@google [1]\n",
      "compare_gan-s3gan_2_5_128x128@@google [1]\n",
      "compare_gan-s3gan_5_128x128@@google [1]\n",
      "compare_gan-ssgan_128x128@@google [1]\n",
      "cond-biggan@@vtab [1]\n",
      "delf@@google [1]\n",
      "efficientnet-b0-classification@@google [1]\n",
      "efficientnet-b0-feature-vector@@google [1]\n",
      "efficientnet-b1-classification@@google [1]\n",
      "efficientnet-b1-feature-vector@@google [1]\n",
      "efficientnet-b2-classification@@google [1]\n",
      "efficientnet-b2-feature-vector@@google [1]\n",
      "efficientnet-b3-classification@@google [1]\n",
      "efficientnet-b3-feature-vector@@google [1]\n",
      "efficientnet-b4-classification@@google [1]\n",
      "efficientnet-b4-feature-vector@@google [1]\n",
      "efficientnet-b5-classification@@google [1]\n",
      "efficientnet-b5-feature-vector@@google [1]\n",
      "efficientnet-b6-classification@@google [1]\n",
      "efficientnet-b6-feature-vector@@google [1]\n",
      "efficientnet-b7-classification@@google [1]\n",
      "efficientnet-b7-feature-vector@@google [1]\n",
      "efficientnet-lite0-classification@@tensorflow [1]\n",
      "efficientnet-lite0-feature-vector@@tensorflow [1]\n",
      "efficientnet-lite1-classification@@tensorflow [1]\n",
      "efficientnet-lite1-feature-vector@@tensorflow [1]\n",
      "efficientnet-lite2-classification@@tensorflow [1]\n",
      "efficientnet-lite2-feature-vector@@tensorflow [1]\n",
      "efficientnet-lite3-classification@@tensorflow [1]\n",
      "efficientnet-lite3-feature-vector@@tensorflow [1]\n",
      "efficientnet-lite4-classification@@tensorflow [1]\n",
      "efficientnet-lite4-feature-vector@@tensorflow [1]\n",
      "elmo@@google [1, 3, 2]\n",
      "esrgan-tf2@@captain-pool [1]\n",
      "exemplar@@vtab [1]\n",
      "faster_rcnn-openimages_v4-inception_resnet_v2@@google [1]\n",
      "ganeval-cifar10-convnet@@deepmind [1]\n",
      "i3d-kinetics-400@@deepmind [1]\n",
      "i3d-kinetics-600@@deepmind [1]\n",
      "image_augmentation-crop_color@@google [1]\n",
      "image_augmentation-crop_rotate_color@@google [1]\n",
      "image_augmentation-flipx_crop_rotate_color@@google [1]\n",
      "image_augmentation-nas_cifar@@google [1]\n",
      "image_augmentation-nas_imagenet@@google [1]\n",
      "image_augmentation-nas_svhn@@google [1]\n",
      "imagenet-amoebanet_a_n18_f448-classification@@google [1]\n",
      "imagenet-amoebanet_a_n18_f448-feature_vector@@google [1]\n",
      "imagenet-inception_resnet_v2-classification@@google [1, 3, 4]\n",
      "imagenet-inception_resnet_v2-feature_vector@@google [1, 3, 4]\n",
      "imagenet-inception_v1-classification@@google [1, 3, 4]\n",
      "imagenet-inception_v1-feature_vector@@google [1, 3, 4]\n",
      "imagenet-inception_v2-classification@@google [1, 3, 4]\n",
      "imagenet-inception_v2-feature_vector@@google [1, 3, 4]\n",
      "imagenet-inception_v3-classification@@google [1, 3, 4]\n",
      "imagenet-inception_v3-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_025_128-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_025_128-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_025_128-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_025_128-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_025_160-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_025_160-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_025_160-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_025_160-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_025_192-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_025_192-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_025_192-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_025_192-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_025_224-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_025_224-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_025_224-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_025_224-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_050_128-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_050_128-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_050_128-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_050_128-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_050_160-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_050_160-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_050_160-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_050_160-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_050_192-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_050_192-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_050_192-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_050_192-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_050_224-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_050_224-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_050_224-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_050_224-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_075_128-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_075_128-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_075_128-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_075_128-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_075_160-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_075_160-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_075_160-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_075_160-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_075_192-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_075_192-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_075_192-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_075_192-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_075_224-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_075_224-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_075_224-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_075_224-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_100_128-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_100_128-feature_vector@@google [1, 4, 3]\n",
      "imagenet-mobilenet_v1_100_128-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_100_128-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_100_160-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_100_160-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_100_160-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_100_160-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_100_192-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_100_192-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_100_192-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_100_192-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v1_100_224-classification@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_100_224-feature_vector@@google [1, 3, 4]\n",
      "imagenet-mobilenet_v1_100_224-quantops-classification@@google [1, 3]\n",
      "imagenet-mobilenet_v1_100_224-quantops-feature_vector@@google [1, 3]\n",
      "imagenet-mobilenet_v2_035_128-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_035_128-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_035_160-classification@@google [1, 3, 4, 2]\n",
      "imagenet-mobilenet_v2_035_160-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_035_192-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_035_192-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_035_224-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_035_224-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_035_96-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_035_96-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_050_128-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_050_128-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_050_160-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_050_160-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_050_192-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_050_192-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_050_224-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_050_224-feature_vector@@google [2, 3, 4, 1]\n",
      "imagenet-mobilenet_v2_050_96-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_050_96-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_075_128-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_075_128-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_075_160-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_075_160-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_075_192-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_075_192-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_075_224-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_075_224-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_075_96-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_075_96-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_100_128-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_100_128-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_100_160-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_100_160-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_100_192-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_100_192-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_100_224-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_100_224-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_100_96-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_100_96-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_130_224-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_130_224-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_140_224-classification@@google [1, 2, 3, 4]\n",
      "imagenet-mobilenet_v2_140_224-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-nasnet_large-classification@@google [1, 3, 4]\n",
      "imagenet-nasnet_large-feature_vector@@google [1, 3, 4]\n",
      "imagenet-nasnet_mobile-classification@@google [1, 3, 4]\n",
      "imagenet-nasnet_mobile-feature_vector@@google [1, 3, 4]\n",
      "imagenet-pnasnet_large-classification@@google [1, 2, 3, 4]\n",
      "imagenet-pnasnet_large-feature_vector@@google [1, 2, 3, 4]\n",
      "imagenet-resnet_v1_101-classification@@google [1, 3, 4]\n",
      "imagenet-resnet_v1_101-feature_vector@@google [1, 3, 4]\n",
      "imagenet-resnet_v1_152-classification@@google [1, 3, 4]\n",
      "imagenet-resnet_v1_152-feature_vector@@google [1, 3, 4]\n",
      "imagenet-resnet_v1_50-classification@@google [1, 3, 4]\n",
      "imagenet-resnet_v1_50-feature_vector@@google [1, 3, 4]\n",
      "imagenet-resnet_v2_101-classification@@google [1, 3, 4]\n",
      "imagenet-resnet_v2_101-feature_vector@@google [1, 3, 4]\n",
      "imagenet-resnet_v2_152-classification@@google [1, 3, 4]\n",
      "imagenet-resnet_v2_152-feature_vector@@google [1, 3, 4]\n",
      "imagenet-resnet_v2_50-classification@@google [1, 3, 4]\n",
      "imagenet-resnet_v2_50-feature_vector@@google [1, 3, 4]\n",
      "inaturalist-inception_v3-feature_vector@@google [1, 3, 4]\n",
      "jigsaw@@vtab [1]\n",
      "llr-pretrain-adv-latents@@deepmind [1]\n",
      "llr-pretrain-adv-linear@@deepmind [1]\n",
      "local-linearity-cifar10@@deepmind [1]\n",
      "local-linearity-imagenet@@deepmind [1]\n",
      "magenta-arbitrary-image-stylization-v1-256@@google [1, 2]\n",
      "mil-nce-i3d@@deepmind [1]\n",
      "mil-nce-s3d@@deepmind [1]\n",
      "nnlm-de-dim128-with-normalization@@google [1, 2]\n",
      "nnlm-de-dim128@@google [1, 2]\n",
      "nnlm-de-dim50-with-normalization@@google [2, 1]\n",
      "nnlm-de-dim50@@google [1, 2]\n",
      "nnlm-en-dim128-with-normalization@@google [1, 2]\n",
      "nnlm-en-dim128@@google [1, 2]\n",
      "nnlm-en-dim50-with-normalization@@google [2, 1]\n",
      "nnlm-en-dim50@@google [1, 2]\n",
      "nnlm-es-dim128-with-normalization@@google [1, 2]\n",
      "nnlm-es-dim128@@google [1, 2]\n",
      "nnlm-es-dim50-with-normalization@@google [1, 2]\n",
      "nnlm-es-dim50@@google [1, 2]\n",
      "nnlm-id-dim128-with-normalization@@google [1, 2]\n",
      "nnlm-id-dim128@@google [1, 2]\n",
      "nnlm-id-dim50-with-normalization@@google [1, 2]\n",
      "nnlm-id-dim50@@google [1, 2]\n",
      "nnlm-ja-dim128-with-normalization@@google [1, 2]\n",
      "nnlm-ja-dim128@@google [1, 2]\n",
      "nnlm-ja-dim50-with-normalization@@google [1, 2]\n",
      "nnlm-ja-dim50@@google [2, 1]\n",
      "nnlm-ko-dim128-with-normalization@@google [1, 2]\n",
      "nnlm-ko-dim128@@google [1, 2]\n",
      "nnlm-ko-dim50-with-normalization@@google [1, 2]\n",
      "nnlm-ko-dim50@@google [1, 2]\n",
      "nnlm-zh-dim128-with-normalization@@google [1, 2]\n",
      "nnlm-zh-dim128@@google [1, 2]\n",
      "nnlm-zh-dim50-with-normalization@@google [1, 2]\n",
      "nnlm-zh-dim50@@google [1, 2]\n",
      "nonsemantic-speech-benchmark-trill-distilled@@google [1]\n",
      "nonsemantic-speech-benchmark-trill@@google [1]\n",
      "nq@@nascarr [1]\n",
      "object_detection-mobile_object_localizer_v1@@google [1]\n",
      "openimages_v4-ssd-mobilenet_v2@@google [1]\n",
      "planet-vision-classifier-planet_v2@@google [1]\n",
      "progan-128@@google [1]\n",
      "random-nnlm-en-dim128@@google [1]\n",
      "random-nnlm-en-dim50@@google [1]\n",
      "relative-patch-location@@vtab [1]\n",
      "remote_sensing-bigearthnet-resnet50@@google [1]\n",
      "remote_sensing-eurosat-ms-resnet50@@google [1]\n",
      "remote_sensing-eurosat-resnet50@@google [1]\n",
      "remote_sensing-resisc45-resnet50@@google [1]\n",
      "remote_sensing-so2sat-ms-resnet50@@google [1]\n",
      "remote_sensing-so2sat-resnet50@@google [1]\n",
      "remote_sensing-uc_merced-resnet50@@google [1]\n",
      "resnet_50-classification@@tensorflow [1]\n",
      "resnet_50-feature_vector@@tensorflow [1]\n",
      "rotation@@vtab [1]\n",
      "semi-exemplar-10@@vtab [1]\n",
      "semi-rotation-10@@vtab [1]\n",
      "speech_embedding@@google [1]\n",
      "spice@@google [1]\n",
      "spiral-default-fluid-gansn-celebahq64-gen-19steps@@deepmind [1]\n",
      "spiral-default-wgangp-celebahq64-gen-19steps-agent1@@deepmind [1]\n",
      "spiral-default-wgangp-celebahq64-gen-19steps-agent2@@deepmind [1]\n",
      "spiral-default-wgangp-celebahq64-gen-19steps-agent3@@deepmind [1]\n",
      "spiral-default-wgangp-celebahq64-gen-19steps-agent4@@deepmind [1]\n",
      "spiral-default-wgangp-celebahq64-gen-19steps-agent5@@deepmind [1]\n",
      "spiral-default-wgangp-celebahq64-gen-19steps-agent6@@deepmind [1]\n",
      "spiral-default-wgangp-celebahq64-gen-19steps-agent7@@deepmind [1]\n",
      "spiral-default-wgangp-celebahq64-gen-19steps-agent8@@deepmind [1]\n",
      "spiral-default-wgangp-celebahq64-gen-19steps-agent9@@deepmind [1]\n",
      "sup-100@@vtab [1]\n",
      "sup-exemplar-100@@vtab [1]\n",
      "sup-rotation-100@@vtab [1]\n",
      "svampeatlas-vision-embedder-fungi_V1@@google [1]\n",
      "tf2-preview-gnews-swivel-20dim-with-oov@@google [1]\n",
      "tf2-preview-gnews-swivel-20dim@@google [1]\n",
      "tf2-preview-inception_v3-classification@@google [2, 3, 4, 1]\n",
      "tf2-preview-inception_v3-feature_vector@@google [2, 3, 4, 1]\n",
      "tf2-preview-mobilenet_v2-classification@@google [1, 2, 3, 4]\n",
      "tf2-preview-mobilenet_v2-feature_vector@@google [1, 2, 3, 4]\n",
      "tf2-preview-nnlm-de-dim128-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-de-dim128@@google [1]\n",
      "tf2-preview-nnlm-de-dim50-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-de-dim50@@google [1]\n",
      "tf2-preview-nnlm-en-dim128-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-en-dim128@@google [1]\n",
      "tf2-preview-nnlm-en-dim50-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-en-dim50@@google [1]\n",
      "tf2-preview-nnlm-es-dim128-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-es-dim128@@google [1]\n",
      "tf2-preview-nnlm-es-dim50-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-es-dim50@@google [1]\n",
      "tf2-preview-nnlm-id-dim128-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-id-dim128@@google [1]\n",
      "tf2-preview-nnlm-id-dim50-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-id-dim50@@google [1]\n",
      "tf2-preview-nnlm-ja-dim128-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-ja-dim128@@google [1]\n",
      "tf2-preview-nnlm-ja-dim50-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-ja-dim50@@google [1]\n",
      "tf2-preview-nnlm-ko-dim128-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-ko-dim128@@google [1]\n",
      "tf2-preview-nnlm-ko-dim50-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-ko-dim50@@google [1]\n",
      "tf2-preview-nnlm-zh-dim128-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-zh-dim128@@google [1]\n",
      "tf2-preview-nnlm-zh-dim50-with-normalization@@google [1]\n",
      "tf2-preview-nnlm-zh-dim50@@google [1]\n",
      "tf2nq@@prvi [1]\n",
      "tfgan-eval-inception@@tensorflow [1]\n",
      "tfgan-eval-mnist-logits@@tensorflow [1]\n",
      "tweening_conv3d_bair@@google [1]\n",
      "tweening_conv3d_kth@@google [1]\n",
      "uncond-biggan@@vtab [1]\n",
      "unet-industrial-class_10@@nvidia [1]\n",
      "unet-industrial-class_1@@nvidia [1]\n",
      "unet-industrial-class_2@@nvidia [1]\n",
      "unet-industrial-class_3@@nvidia [1]\n",
      "unet-industrial-class_4@@nvidia [1]\n",
      "unet-industrial-class_5@@nvidia [1]\n",
      "unet-industrial-class_6@@nvidia [1]\n",
      "unet-industrial-class_7@@nvidia [1]\n",
      "unet-industrial-class_8@@nvidia [1]\n",
      "unet-industrial-class_9@@nvidia [1]\n",
      "universal-sentence-encoder-large@@google [1, 2, 3, 4, 5]\n",
      "universal-sentence-encoder-lite@@google [1, 2]\n",
      "universal-sentence-encoder-multilingual-large@@google [1, 2, 3]\n",
      "universal-sentence-encoder-multilingual-qa@@google [1, 2, 3]\n",
      "universal-sentence-encoder-multilingual@@google [1, 2, 3]\n",
      "universal-sentence-encoder-qa@@google [1, 2, 3]\n",
      "universal-sentence-encoder-xling-en-de@@google [1]\n",
      "universal-sentence-encoder-xling-en-es@@google [1]\n",
      "universal-sentence-encoder-xling-en-fr@@google [1]\n",
      "universal-sentence-encoder-xling-many@@google [1]\n",
      "universal-sentence-encoder@@google [1, 2, 3, 4]\n",
      "unsupervised-adversarial-training-cifar10-wrn_106@@deepmind [1]\n",
      "vae@@vtab [1]\n",
      "videoflow-encoder@@google [1]\n",
      "videoflow-evaluator@@google [1]\n",
      "videoflow-generator@@google [1]\n",
      "vision-classifier-fungi_mobile_V1@@svampeatlas [1]\n",
      "vision-classifier-imet_attributes_V1@@metmuseum [1]\n",
      "vision-detector-megadetector_V3@@microsoft-ai-for-earth [1]\n",
      "vision-embedder-fungi_V2@@svampeatlas [1]\n",
      "vision-embedder-inaturalist_V2@@inaturalist [1]\n",
      "wae-gan@@vtab [1]\n",
      "wae-mmd@@vtab [1]\n",
      "wae-ukl@@vtab [1]\n",
      "There are in total 741 versions in TFHub\n"
     ]
    }
   ],
   "source": [
    "def see_how_many_models_have_multiple_versions():\n",
    "    raw_file_path = \"J:/ModelStoreData/AIHub/2020-03-10/TFHubModule_Info.json\"\n",
    "    with open(raw_file_path, \"r\") as rfp:\n",
    "        modules = json.load(rfp)\n",
    "    \n",
    "    specialFormatModules = []\n",
    "    \n",
    "    module_names = {}\n",
    "    for module in modules:\n",
    "        module_name = module[\"name\"]\n",
    "        version_num = module[\"version\"]\n",
    "        author = None\n",
    "        for l in module[\"info_NormalDeployment\"][-1][42]:\n",
    "            if l[0] == \"publisher\":\n",
    "                author = l[1][0][0]\n",
    "                break\n",
    "        module_name = \"{}@@{}\".format(module_name, author)\n",
    "        \n",
    "#                 print(l[1][1])\n",
    "        \n",
    "        moduleFormat = module[\"info_NormalDeployment\"][-1][-11]\n",
    "        ## 我们只关注默认格式是这两种的package，同时，我们也只关注它们的这两种格式的模型。\n",
    "        ## 举例，有的package默认格式就是TFLite，TFJS之类，没有普通格式hub和tf2sm，那我们不管这些package。 https://tfhub.dev/tensorflow/coral-model/mobilenet_v2_1.0_224_quantized/1/default/1\n",
    "        ## 同时，有一个package有tf2sm格式，同时还有tflite和tfjs格式，那我们只关注tf2sm格式，不关注另外的格式。https://tfhub.dev/google/imagenet/mobilenet_v2_075_96/feature_vector/4\n",
    "        ## 不知道这样可不可以。\n",
    "        if moduleFormat not in (\"saved_model_2\", \"hub\", \"saved_model\"):\n",
    "            specialFormatModules.append(module_name)\n",
    "            continue\n",
    "            \n",
    "        if module_name not in module_names:\n",
    "            module_names[module_name] = [version_num]\n",
    "        else:\n",
    "            module_names[module_name].append(version_num)\n",
    "\n",
    "    keys = list(module_names.keys())\n",
    "    keys.sort()\n",
    "    print(\"there are {} modules in TFHub\\n\\n\\n\".format(len(keys)))\n",
    "    versionCount = 0\n",
    "    for name in keys:\n",
    "        v_list = module_names[name]\n",
    "        versionCount += len(v_list)\n",
    "        print(name, v_list)\n",
    "#         if len(v_list) > 1:\n",
    "#             print(name, v_list)\n",
    "    print(\"There are in total {} versions in TFHub\".format(versionCount))\n",
    "#     print(\"Those modules are not in general formats: \")\n",
    "#     pprint(set(specialFormatModules))\n",
    "    \n",
    "    \n",
    "see_how_many_models_have_multiple_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a CSV table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_a_csv_table():\n",
    "    raw_file_path = \"J:/ModelStoreData/AIHub/2020-03-10/TFHubModule_Info.json\"\n",
    "    with open(raw_file_path, \"r\") as rfp:\n",
    "        modules = json.load(rfp)\n",
    "    with open(\"rst.csv\", \"w\") as rst:\n",
    "        rst.write(\"{},{},{},{},{},{},{}\\n\".format(\"name\", \"version\", \"dataset\", \"fmt\", \"module_type\", \"network_architecture\", \"author\"))\n",
    "        for module in modules:\n",
    "            name = module[\"name\"]\n",
    "            version = module[\"version\"]\n",
    "            author = None\n",
    "            for l in module[\"info_NormalDeployment\"][-1][42]:\n",
    "                if l[0] == \"publisher\":\n",
    "                    author = l[1][0][0]\n",
    "                    break\n",
    "            name = \"{}@@{}\".format(name, author)\n",
    "            \n",
    "            ## 排除掉一些特异格式的模型。\n",
    "            moduleFormat = module[\"info_NormalDeployment\"][-1][-11]\n",
    "            if moduleFormat not in (\"saved_model_2\", \"hub\", \"saved_model\"):\n",
    "                continue\n",
    "            \n",
    "            labels = module[\"info_NormalDeployment\"][1][19]\n",
    "#             print(labels)\n",
    "            dataset = None\n",
    "            fmt = moduleFormat #None\n",
    "            module_type = None\n",
    "            network_architecture = None\n",
    "            for _ in labels:\n",
    "                label = _.replace(\",\", \"|\")\n",
    "                if \"dataset\" in label and dataset == None:\n",
    "                    dataset = label.split(\":\")[1]\n",
    "#                 elif \"format\" in label:\n",
    "#                     print(name, version)\n",
    "#                     if \"hub\" in label:\n",
    "#                         fmt = \"hub\"\n",
    "#                         continue\n",
    "#                     else: \n",
    "#                         fmt = label.split(\":\")[1]\n",
    "                elif \"module-type\" in label and module_type == None:\n",
    "                    module_type = label.split(\":\")[1]\n",
    "                elif \"network-architecture\" in label and network_architecture == None:\n",
    "                    network_architecture = label.split(\":\")[1]\n",
    "            # print(\"{} {} {} {} {} {}\\n\".format(name, version, dataset, fmt, module_type, network_architecture))\n",
    "            rst.write(\"{},{},{},{},{},{},{}\\n\".format(name, version, dataset, fmt, module_type, network_architecture, author))\n",
    "generate_a_csv_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look into the CSV file, some Database operations\n",
    "\n",
    "It seems that there are around `62%` modules have multiple versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are in total 383 modules.\n",
      "                                        name  versions\n",
      "0  Wiki-words-250-with-normalization@@google         2\n",
      "1                     Wiki-words-250@@google         2\n",
      "2  Wiki-words-500-with-normalization@@google         2\n",
      "3                     Wiki-words-500@@google         2\n",
      "4                        albert_base@@google         3\n",
      "\n",
      "There are in total 187 modules have multiple versions.\n"
     ]
    }
   ],
   "source": [
    "def how_many_models_have_multiple_versions():\n",
    "    table = pd.read_csv(\"rst.csv\")\n",
    "    table.head()\n",
    "    ##---------------\n",
    "    df_gp = table.groupby(\"name\", as_index=False)\n",
    "    new_df = df_gp.count().reset_index()\n",
    "    # new_df.head()\n",
    "    print(\n",
    "        \"\\nThere are in total {} modules.\".format(new_df.shape[0])\n",
    "    )\n",
    "    ## --------------------\n",
    "    new_df.drop(columns=[\"index\", \"dataset\", \"fmt\", \"module_type\", \"network_architecture\", \"author\"], inplace=True)\n",
    "    new_df.rename(columns={\"version\": \"versions\"}, inplace=True)\n",
    "    print(new_df.head())\n",
    "    new_df.to_csv(\"how_many_models_have_multiple_versions.csv\", index=False)\n",
    "    print(\n",
    "        \"\\nThere are in total {} modules have multiple versions.\".format(new_df[new_df[\"versions\"] > 1].shape[0])\n",
    "    )\n",
    "    return\n",
    "\n",
    "how_many_models_have_multiple_versions()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        name  fmts\n",
      "0  Wiki-words-250-with-normalization@@google     2\n",
      "1                     Wiki-words-250@@google     2\n",
      "2  Wiki-words-500-with-normalization@@google     2\n",
      "3                     Wiki-words-500@@google     2\n",
      "4                        albert_base@@google     1\n",
      "\n",
      "There are in total 141 modules have multiple formats.\n"
     ]
    }
   ],
   "source": [
    "def how_many_models_have_multiple_format():\n",
    "    table = pd.read_csv(\"rst.csv\")\n",
    "    table.head()\n",
    "    ## --------------------\n",
    "    df_gp = table.groupby([\"name\", \"fmt\"], as_index=False)\n",
    "    new_df = df_gp.count().reset_index()\n",
    "    new_df.drop(columns=[\"index\", \"dataset\", \"module_type\", \"network_architecture\", \"author\"], inplace=True)\n",
    "    # print(new_df[new_df[\"name\"].str.contains(\"classification\")])\n",
    "    ### --------------------\n",
    "    df_gp = new_df.groupby([\"name\"], as_index=False)\n",
    "    new_df = df_gp.count().reset_index()\n",
    "    new_df.drop(columns=[\"index\", \"version\"], inplace=True)\n",
    "    ### ------------------\n",
    "    new_df.rename(columns={\"fmt\": \"fmts\"}, inplace=True)\n",
    "    print(new_df.head())\n",
    "    new_df.to_csv(\"how_many_models_have_multiple_format.csv\", index=False)\n",
    "    print(\n",
    "        \"\\nThere are in total {} modules have multiple formats.\".format(new_df[new_df[\"fmts\"] > 1].shape[0])\n",
    "    )\n",
    "    return\n",
    "\n",
    "how_many_models_have_multiple_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        name  fmts            fmt_str\n",
      "0  Wiki-words-250-with-normalization@@google     2  hub,saved_model_2\n",
      "1                     Wiki-words-250@@google     2  hub,saved_model_2\n",
      "2  Wiki-words-500-with-normalization@@google     2  hub,saved_model_2\n",
      "3                     Wiki-words-500@@google     2  hub,saved_model_2\n",
      "4                        albert_base@@google     1                hub\n",
      "\n",
      "There are in total 141 modules have multiple formats.\n"
     ]
    }
   ],
   "source": [
    "def how_many_models_have_multiple_format1():\n",
    "    table = pd.read_csv(\"rst.csv\")\n",
    "    table.head()\n",
    "    ## --------------------\n",
    "    df_gp = table.groupby([\"name\", \"fmt\"], as_index=False)\n",
    "    new_df = df_gp.count().reset_index()\n",
    "    new_df.drop(columns=[\"index\", \"dataset\", \"module_type\", \"network_architecture\", \"author\"], inplace=True)\n",
    "    # print(new_df[new_df[\"name\"].str.contains(\"classification\")])\n",
    "    ### --------------------\n",
    "    df_gp = new_df.groupby([\"name\"], as_index=False)\n",
    "    new_df = df_gp.count().reset_index()\n",
    "    new_df.drop(columns=[\"index\", \"version\"], inplace=True)\n",
    "    new_df[\"fmt_str\"] = df_gp['fmt'].apply(','.join)\n",
    "    ### ------------------\n",
    "    new_df.rename(columns={\"fmt\": \"fmts\"}, inplace=True)\n",
    "    print(new_df.head())\n",
    "    new_df.to_csv(\"how_many_models_have_multiple_format.csv\", index=False)\n",
    "    print(\n",
    "        \"\\nThere are in total {} modules have multiple formats.\".format(new_df[new_df[\"fmts\"] > 1].shape[0])\n",
    "    )\n",
    "    return\n",
    "\n",
    "how_many_models_have_multiple_format1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        name  \\\n",
      "0  Wiki-words-250-with-normalization@@google   \n",
      "1                     Wiki-words-250@@google   \n",
      "2  Wiki-words-500-with-normalization@@google   \n",
      "3                     Wiki-words-500@@google   \n",
      "4                        albert_base@@google   \n",
      "\n",
      "                                             dataset     module_type  \\\n",
      "0                                          Wikipedia  text-embedding   \n",
      "1                                          Wikipedia  text-embedding   \n",
      "2                                          Wikipedia  text-embedding   \n",
      "3                                          Wikipedia  text-embedding   \n",
      "4  Wikipedia| BooksCorpus| Stories| CommonCrawl| ...  text-embedding   \n",
      "\n",
      "  network_architecture  author  \n",
      "0   word2vec skip-gram       2  \n",
      "1   word2vec skip-gram       2  \n",
      "2   word2vec skip-gram       2  \n",
      "3   word2vec skip-gram       2  \n",
      "4          Transformer       3  \n",
      "\n",
      "There are in total 383 modules have other fields.\n"
     ]
    }
   ],
   "source": [
    "def module_type_and_network_architecture_of_the_modules():\n",
    "    table = pd.read_csv(\"rst.csv\")\n",
    "    table.head()\n",
    "    ## --------------------\n",
    "    df_gp = table.groupby([\"name\", \"dataset\", \"module_type\", \"network_architecture\"], as_index=False)\n",
    "    new_df = df_gp.count().reset_index()\n",
    "    new_df.drop(columns=[\"index\", \"fmt\", \"version\"], inplace=True)\n",
    "    ## --------------------\n",
    "    print(new_df.head())\n",
    "    new_df.to_csv(\"module_type_and_network_architecture_of_the_modules.csv\", index=False)\n",
    "    print(\n",
    "        \"\\nThere are in total {} modules have other fields.\".format(new_df.shape[0])\n",
    "    )\n",
    "    return\n",
    "\n",
    "module_type_and_network_architecture_of_the_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the CSV files and see some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know why in the merged result, universal-sentence-encoder-lite will have an extra line. 奇哉怪哉。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>versions</th>\n",
       "      <th>fmts</th>\n",
       "      <th>fmt_str</th>\n",
       "      <th>dataset</th>\n",
       "      <th>module_type</th>\n",
       "      <th>network_architecture</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wiki-words-250-with-normalization@@google</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>hub,saved_model_2</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>text-embedding</td>\n",
       "      <td>word2vec skip-gram</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wiki-words-250@@google</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>hub,saved_model_2</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>text-embedding</td>\n",
       "      <td>word2vec skip-gram</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wiki-words-500-with-normalization@@google</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>hub,saved_model_2</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>text-embedding</td>\n",
       "      <td>word2vec skip-gram</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wiki-words-500@@google</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>hub,saved_model_2</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>text-embedding</td>\n",
       "      <td>word2vec skip-gram</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert_base@@google</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>hub</td>\n",
       "      <td>Wikipedia| BooksCorpus| Stories| CommonCrawl| ...</td>\n",
       "      <td>text-embedding</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        name  versions  fmts  \\\n",
       "0  Wiki-words-250-with-normalization@@google         2     2   \n",
       "1                     Wiki-words-250@@google         2     2   \n",
       "2  Wiki-words-500-with-normalization@@google         2     2   \n",
       "3                     Wiki-words-500@@google         2     2   \n",
       "4                        albert_base@@google         3     1   \n",
       "\n",
       "             fmt_str                                            dataset  \\\n",
       "0  hub,saved_model_2                                          Wikipedia   \n",
       "1  hub,saved_model_2                                          Wikipedia   \n",
       "2  hub,saved_model_2                                          Wikipedia   \n",
       "3  hub,saved_model_2                                          Wikipedia   \n",
       "4                hub  Wikipedia| BooksCorpus| Stories| CommonCrawl| ...   \n",
       "\n",
       "      module_type network_architecture  author  \n",
       "0  text-embedding   word2vec skip-gram       2  \n",
       "1  text-embedding   word2vec skip-gram       2  \n",
       "2  text-embedding   word2vec skip-gram       2  \n",
       "3  text-embedding   word2vec skip-gram       2  \n",
       "4  text-embedding          Transformer       3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_the_csv_files(): \n",
    "    table1 = pd.read_csv(\"how_many_models_have_multiple_versions.csv\")\n",
    "    table2 = pd.read_csv(\"how_many_models_have_multiple_format.csv\")\n",
    "    table3 = pd.read_csv(\"module_type_and_network_architecture_of_the_modules.csv\")\n",
    "\n",
    "    _1 = pd.merge(table1, table2, \"inner\", on=\"name\")\n",
    "    _2 = pd.merge(_1, table3, \"inner\", on=\"name\")\n",
    "\n",
    "    merged_table = _2\n",
    "    print(merged_table.shape[0])\n",
    "    merged_table.to_csv(\"final_info.csv\", index=False)\n",
    "    return merged_table\n",
    "\n",
    "merged_table = merge_the_csv_files()\n",
    "merged_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_merged_table(merged_table):\n",
    "    merged_table = merged_table[[\"module_type\", \"network_architecture\", \"dataset\", \"name\", \"versions\", \"fmts\", \"fmt_str\", \"author\"]]\n",
    "    merged_table.sort_values([\"module_type\", \"network_architecture\", \"dataset\", \"name\"], ascending=True, inplace=True)\n",
    "#     merged_table.sort_values([\"name\"], ascending=True, inplace=True)\n",
    "    merged_table.to_csv(\"sorted.csv\", index=False)\n",
    "    return merged_table\n",
    "\n",
    "sorted_table = sort_merged_table(merged_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 从模型的名字里面把作者拆出来。\n",
    "def getAuthors(sorted_table):\n",
    "    sorted_table[\"_\"], sorted_table[\"author\"] = sorted_table[\"name\"].str.split(\"@@\").str\n",
    "    sorted_table.drop(columns=[\"_\"], inplace=True)\n",
    "    sorted_table.to_csv(\"sorted-withAuthor.csv\", index=False)\n",
    "    return\n",
    "getAuthors(sorted_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image-feature-vector           111\n",
       "text-embedding                  99\n",
       "image-classification            88\n",
       "image-generator                 30\n",
       "image-segmentation              10\n",
       "image-rnn-agent                 10\n",
       "image-augmentation               6\n",
       "image-classifier                 5\n",
       "image-object-detection           4\n",
       "audio-embedding                  3\n",
       "text-question-answering          3\n",
       "video-generation                 3\n",
       "video-text                       2\n",
       "video-classification             2\n",
       "video-generator                  2\n",
       "image-others                     1\n",
       "image-style-transfer             1\n",
       "audio-pitch-extraction           1\n",
       "image-super-resolution           1\n",
       "image-classification-logits      1\n",
       "Name: module_type, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_table[\"module_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
